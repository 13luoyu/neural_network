

# 数值稳定性
# 一个神经网络有d层，那么如果定义h(t) = ft(h(t-1)) and y = l*fd*...*f1(x)
# 计算损失l对于参数w(t)的梯度：dl/dw(t) = dl/dh(d) * dh(d)/dh(d-1)*...*dh(t)/dw(t)
# 梯度爆炸：如果d-t很大，该值是连乘，也会很大（如使用relu激活函数时）
# 问题：1可能值超出值域，2对学习率的太大和太小敏感
# 梯度消失：如果d-t很大，结果会很小（sigmoid激活函数）
# 问题：1梯度值变成0，2训练没有进展，无论学习率为多少，3对于深度神经网络尤为严重

# 让训练更加稳定
# 让梯度值在合理的范围内。方法有：1乘法变加法，2梯度归一化和梯度裁剪，3合理的权重初始化和激活函数
# 3
# 将每层的输出和梯度看作随机变量，让它们的均值和方差保持一致
# 经证明，需要满足n(t-1) * γ(t) = 1, n(t) * γ(t) = 1，其中n(t)为t层神经元数，γ(t)为t层方差
# Xavier方法使得γ(t)(n(t-1)+n(t))/2 = 1, γ(t) = 2/(n(t-1)+n(t))
# 因此，w取值可以正态分布N( 0, 根号下2/(n(t-1)+n(t)) )
# 已知[a,b]间均匀分布方差是(b-a)^2/12，则xavier初始化的实现就是下面的均匀分布
# W ~ U[-根号6 / 根号(n(t-1)+n(t)), 根号6 / 根号(n(t-1)+n(t))]

# 对于激活函数，最好的激活函数是σ(x) = x
# tanh 和 relu 函数在0附近近似，如果想使用sigmoid，虽好使用4*sigmoid-2
























